{
	"jobConfig": {
		"name": "json_to_parquet",
		"description": "",
		"role": "arn:aws:iam::893648891401:role/service-role/AWSGlueServiceRole-json_to_parquet",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "json_.py",
		"scriptLocation": "s3://aws-glue-assets-893648891401-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-10-06T18:35:27.156Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-893648891401-us-east-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-893648891401-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import coalesce, col, lit, explode, expr, size, round\nfrom pyspark.sql.types import StringType, FloatType, IntegerType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n#initialize Glue\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nglueContext = GlueContext(SparkContext.getOrCreate())\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\ns3_bucket = \"s3://bucket-name\" #specify s3 bucket here\n\n#read from source\ndatasource0 = glueContext.create_dynamic_frame.from_catalog(database=\"weather\", table_name=\"raw\")\n\n#optional code for testing with single json file\n\"\"\"\nsingle_file_path = f\"{s3_bucket}/raw/weather_data_202301.json\"\n\n# Reading from the single JSON file using from_options\ndatasource0 = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\": [single_file_path]},\n    format=\"json\"\n)\n\"\"\"\n\ndf = datasource0.toDF()\n\n#flatten observations array\nflat_df = df.withColumn(\"observation\", explode(col(\"observations.observations\")))\nflat_df = flat_df.withColumn(\"date\", col(\"observation.date\"))\n\n# aggregate averages\nfields_to_aggregate_1 = [\n    \"temp\"\n    , \"rh\"\n    , \"feels_like\"\n    , \"wc\"\n    , \"heat_index\"\n    , \"wdir\"\n    , \"dewPt\"\n    , \"wspd\"\n    , \"uv_index\"\n]\n\nfor field in fields_to_aggregate_1:\n    flat_df = flat_df.withColumn(\n        field, \n        expr(f\"\"\"\n            IF(size(observation.{field}) > 0, \n            aggregate(observation.{field}, 0, (acc, x) -> acc + x) / size(observation.{field}), \n            NULL)\n        \"\"\")\n    )\n    \nflat_df = flat_df.withColumn(\n    \"gust\",\n    F.expr(\"\"\"\n        CASE\n            WHEN size(filter(observation.gust, x -> x IS NOT NULL)) > 0 \n            THEN aggregate(\n                filter(observation.gust, x -> x IS NOT NULL), \n                0D, \n                (acc, x) -> acc + x\n            ) / size(filter(observation.gust, x -> x IS NOT NULL))\n            ELSE NULL\n        END\n    \"\"\")\n)\n \nflat_df = flat_df.withColumn(\n    \"pressure\", \n    expr(\"\"\"\n        IF(size(observation.pressure) > 0,\n        aggregate(\n            transform(observation.pressure, x -> CAST(x AS DOUBLE)), \n            CAST(0.0 AS DOUBLE), \n            (acc, x) -> acc + x\n        ) / size(observation.pressure),\n        NULL)\n    \"\"\")\n)\n\nflat_df = flat_df.withColumn(\n    \"precip_hrly\",\n    F.expr(\"aggregate(transform(observation.precip_hrly, x -> coalesce(x.int, 0)), 0, (acc, x) -> acc + x)\")\n)\n\nflat_df = flat_df.withColumn(\n    \"snow_hrly\",\n    F.expr(\"aggregate(observation.snow_hrly, 0D, (acc, x) -> acc + x)\")\n)\n\nflat_df = flat_df.withColumn(\n    \"vis\",\n    F.expr(\"aggregate(transform(observation.vis, x -> coalesce(x.int, 0)), 0, (acc, x) -> acc + x)\")\n)\n\nfields_to_mode = [\n    \"wx_phrase\"\n    , \"uv_desc\"\n    , \"wdir_cardinal\"\n    ]\n\nfor field in fields_to_mode:\n    \n    exploded_df = flat_df.select(\"observation.date\", F.explode(F.col(f\"observation.{field}\")).alias(f\"{field}_exploded\"))\n    counted_df = exploded_df.groupBy(\"date\", f\"{field}_exploded\").count()\n    window = Window.partitionBy(\"date\").orderBy(F.desc(\"count\"))\n    most_common_value_df = counted_df.withColumn(\"rank\", F.row_number().over(window)) \\\n                                     .filter(F.col(\"rank\") == 1) \\\n                                     .select(\"date\", F.col(f\"{field}_exploded\").alias(f\"{field}_mode\"))\n    flat_df = flat_df.drop(f\"{field}\").join(most_common_value_df, \"date\")\n\n#select necessary fields\nflat_df = flat_df.select(\n    coalesce(col(\"observation.key\").cast(StringType()), lit(None)).alias(\"key\"),\n    col(\"observation.date\").alias(\"date\"),\n    #coalesce(col(\"observation.class\").cast(StringType()), lit(None)).alias(\"class\"),\n    #coalesce(col(\"observation.expire_time_gmt\").cast(StringType()), lit(None)).alias(\"expire_time_gmt\"),\n    #coalesce(col(\"observation.obs_id\"), lit(None)).alias(\"obs_id\"),\n    coalesce(col(\"observation.obs_name\").cast(StringType()), lit(None)).alias(\"obs_name\"),\n    #coalesce(col(\"observation.valid_time_gmt\").cast(StringType()), lit(None)).alias(\"valid_time_gmt\"),\n    #coalesce(col(\"observation.day_ind\").cast(StringType()), lit(None)).alias(\"day_ind\"),\n    round(coalesce(col(\"temp\"), lit(None)), 0).cast(IntegerType()).alias(\"temperature\"),\n    #coalesce(col(\"observation.wx_icon\").cast(FloatType()), lit(None)).alias(\"wx_icon\"),\n    #coalesce(col(\"observation.icon_extd\").cast(StringType()), lit(None)).alias(\"icon_extd\"),\n    coalesce(col(\"wx_phrase_mode\").cast(StringType()), lit(None)).alias(\"forecast_desc\"),\n    #coalesce(col(\"observation.pressure_tend\").cast(StringType()), lit(None)).alias(\"pressure_tend\"),\n    round(coalesce(col(\"dewPt\").cast(FloatType()), lit(0)), 2).alias(\"dew_point\"),\n    round(coalesce(col(\"heat_index\").cast(FloatType()), lit(0)), 2).alias(\"heat_index\"),\n    round(coalesce(col(\"rh\").cast(FloatType()), lit(0)), 2).alias(\"relative_humidity\"),\n    round(coalesce(col(\"pressure\").cast(FloatType()), lit(0)), 2).alias(\"pressure\"),\n    #coalesce(col(\"pressure_desc_mode\").cast(StringType()), lit(None)).alias(\"pressure_desc\"),\n    coalesce(col(\"vis\").cast(FloatType()), lit(0)).alias(\"visibility\"),\n    round(coalesce(col(\"wc\").cast(FloatType()), lit(0)), 2).alias(\"wind_chill\"),\n    round(coalesce(col(\"wdir\"), lit(None)), 0).cast(IntegerType()).alias(\"wind_direction\"),\n    coalesce(col(\"wdir_cardinal_mode\").cast(StringType()), lit(None)).alias(\"wind_direction_cardinal\"),\n    round(coalesce(col(\"gust\").cast(FloatType()), lit(0)), 2).alias(\"gust\"),\n    round(coalesce(col(\"wspd\").cast(FloatType()), lit(0)), 2).alias(\"wind_speed\"),\n    #coalesce(col(\"max_temp\").cast(FloatType()), lit(None)).alias(\"max_temp\"),\n    #coalesce(col(\"min_temp\").cast(FloatType()), lit(None)).alias(\"min_temp\"),\n    #coalesce(col(\"precip_total\").cast(FloatType()), lit(None)).alias(\"precipitation\"),\n    round(coalesce(col(\"precip_hrly\").cast(FloatType()), lit(0)), 2).alias(\"total_precipitation\"),\n    round(coalesce(col(\"snow_hrly\").cast(FloatType()), lit(0)), 2).alias(\"total_snow\"),\n    coalesce(col(\"uv_desc_mode\").cast(StringType()), lit(None)).alias(\"UV\"),\n    round(coalesce(col(\"uv_index\").cast(FloatType()), lit(0)), 2).alias(\"UV_index\"),  \n    round(coalesce(col(\"feels_like\"), lit(None)), 0).cast(IntegerType()).alias(\"feels_like\"),\n    #coalesce(col(\"observation.qualifier\").cast(StringType()), lit(None)).alias(\"qualifier\"),\n    #coalesce(col(\"observation.qualifier_svrty\").cast(StringType()), lit(None)).alias(\"qualifier_svrty\"),\n    #coalesce(col(\"observation.blunt_phrase\").cast(StringType()), lit(None)).alias(\"blunt_phrase\"),\n    #coalesce(col(\"observation.terse_phrase\").cast(StringType()), lit(None)).alias(\"terse_phrase\"),\n    #coalesce(col(\"observation.clds\").cast(FloatType()), lit(None)).alias(\"clds\")\n)\n\n#simplify/drop redundant fields\nflat_df = flat_df.withColumn(\"date\", F.expr(\"date[0]\"))\nflat_df = flat_df.withColumn(\"obs_name\", F.expr(\"substring(obs_name, 1, 1)\"))\nflat_df = flat_df.drop(\"key\", \"obs_name\")\n\n#write to s3\nflat_df.write.mode(\"overwrite\").parquet(f\"{s3_bucket}/processed/\")\njob.commit()\n"
}