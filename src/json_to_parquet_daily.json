{
	"jobConfig": {
		"name": "json_to_parquet_daily",
		"description": "",
		"role": "arn:aws:iam::893648891401:role/service-role/AWSGlueServiceRole-json_to_parquet",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "json_to_parquet_daily.py",
		"scriptLocation": "s3://aws-glue-assets-893648891401-us-east-2/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-10-18T18:33:29.690Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-893648891401-us-east-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-893648891401-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import coalesce, col, lit, explode, expr, size, round\nfrom pyspark.sql.types import StringType, FloatType, IntegerType\n\n# Initialize Spark and Glue contexts\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n# Reading from the source table\ndatasource0 = glueContext.create_dynamic_frame.from_catalog(database=\"weather\", table_name=\"raw\")\n\n# Convert to DataFrame for processing\ndf = datasource0.toDF()\n\n# Explode the observations array\nflat_df = df.withColumn(\"observation\", F.explode(F.col(\"observations\")))\n\n# Select relevant fields and rename for consistency\nflat_df = flat_df.select(\n    F.col(\"observation.date\").alias(\"date\"), \n    F.col(\"observation.temp\").alias(\"temperature\"),\n    F.col(\"observation.wx_phrase\").alias(\"forecast_desc\"),\n    F.col(\"observation.dewPt\").cast(\"float\").alias(\"dew_point\"),\n    F.col(\"observation.heat_index\").cast(\"float\").alias(\"heat_index\"),\n    F.col(\"observation.rh\").cast(\"float\").alias(\"relative_humidity\"),\n    F.col(\"observation.pressure\").cast(\"float\").alias(\"pressure\"),\n    F.col(\"observation.vis\").cast(\"float\").alias(\"visibility\"),\n    F.col(\"observation.wc\").cast(\"float\").alias(\"wind_chill\"),\n    F.col(\"observation.wdir\").alias(\"wind_direction\"),\n    F.col(\"observation.wdir_cardinal\").alias(\"wind_direction_cardinal\"),\n    F.col(\"observation.gust\").cast(\"float\").alias(\"gust\"),\n    F.col(\"observation.wspd\").cast(\"float\").alias(\"wind_speed\"),\n    F.col(\"observation.precip_total\").cast(\"float\").alias(\"total_precipitation\"),\n    F.col(\"observation.snow_hrly\").cast(\"float\").alias(\"total_snow\"),\n    F.col(\"observation.uv_desc\").alias(\"UV\"),\n    F.col(\"observation.uv_index\").cast(\"float\").alias(\"UV_index\"),\n    F.col(\"observation.feels_like\").cast(\"float\").alias(\"feels_like\")\n)\n\n# Fill missing values with defaults\nflat_df = flat_df.fillna({\n    \"gust\": 0.0, \n    \"wind_speed\": 0.0, \n    \"total_precipitation\": 0.0, \n    \"total_snow\": 0.0,\n    \"dew_point\": 0.0,\n    \"heat_index\": 0.0,\n    \"relative_humidity\": 0.0,\n    \"pressure\": 0.0,\n    \"visibility\": 0.0,\n    \"wind_chill\": 0.0,\n    \"UV_index\": 0.0,\n    \"feels_like\": 0.0\n})\n\nflat_df = flat_df.filter(F.col(\"date\").isNotNull())\n\n# Group by 'date' and aggregate all fields to create a single row per day\naggregated_df = flat_df.groupBy(\"date\").agg(\n    F.round(F.avg(\"temperature\"), 0).cast(\"integer\").alias(\"temperature\"),\n    F.round(F.avg(\"dew_point\"), 2).alias(\"dew_point\"),\n    F.round(F.avg(\"heat_index\"), 2).alias(\"heat_index\"),\n    F.round(F.avg(\"relative_humidity\"), 2).alias(\"relative_humidity\"),\n    F.round(F.avg(\"pressure\"), 2).alias(\"pressure\"),\n    F.round(F.avg(\"visibility\"), 2).alias(\"visibility\"),\n    F.round(F.avg(\"wind_chill\"), 2).alias(\"wind_chill\"),\n    F.round(F.avg(\"wind_speed\"), 2).alias(\"wind_speed\"),\n    F.round(F.sum(\"total_precipitation\"), 2).alias(\"total_precipitation\"),\n    F.round(F.sum(\"total_snow\"), 2).alias(\"total_snow\"),\n    F.max(\"UV\").alias(\"UV\"),\n    F.round(F.max(\"UV_index\"), 2).alias(\"UV_index\"),\n    F.round(F.avg(\"feels_like\"), 0).cast(\"integer\").alias(\"feels_like\"),\n    F.first(\"wind_direction_cardinal\").alias(\"wind_direction_cardinal\"),\n    F.first(\"forecast_desc\").alias(\"forecast_desc\"),\n    F.max(\"gust\").alias(\"gust\"),\n    F.max(\"wind_direction\").alias(\"wind_direction\")\n)\n\nfinal_df = aggregated_df.select(\n    col(\"date\").cast(StringType()),\n    col(\"temperature\").cast(IntegerType()),\n    col(\"forecast_desc\").cast(StringType()),\n    col(\"dew_point\").cast(FloatType()),\n    col(\"heat_index\").cast(FloatType()),\n    col(\"relative_humidity\").cast(FloatType()),\n    col(\"pressure\").cast(FloatType()),\n    col(\"visibility\").cast(FloatType()),\n    col(\"wind_chill\").cast(FloatType()),\n    col(\"wind_direction\").cast(IntegerType()),\n    col(\"wind_direction_cardinal\").cast(StringType()),\n    col(\"gust\").cast(FloatType()),\n    col(\"wind_speed\").cast(FloatType()),\n    col(\"total_precipitation\").cast(FloatType()),\n    col(\"total_snow\").cast(FloatType()),\n    col(\"UV\").cast(StringType()),\n    col(\"UV_index\").cast(FloatType()),  \n    col(\"feels_like\").cast(IntegerType())\n)\n\n# Write the aggregated data to Parquet in overwrite mode\noutput_path = \"s3://dyls-weather-data/processed/\"\nfinal_df.write.mode(\"append\").parquet(output_path)\n\nprint(\"Job completed successfully!\")\n"
}